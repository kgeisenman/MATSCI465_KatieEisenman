# MATSCI465_KatieEisenman
# Weeks 3 and 4 Assignment

This week's assignment walked us through classical image optimizing and segmentation, how to use basic machine learning techniques to extract features from images, and how to train and use deep learning networks to classify and segment images. These techniques are foundational to TEM image analysis. The dataset used was DOPAD, the Dataset of nanoPArticle Detection, which consists of TEM nanoparticle images that are widely used.

For the classical image analysis, I first applied noise reduction using median filtering, enhanced contrast with CLAHE, then segmented features using Otsu thresholding and then the Watershed algorithm.  I used the skimage.measure.regionprops library to quanitfy particle area, equivalent diameter, eccentricity, and other relevant properties. Generally, I found that the classical image optimization was straightforward and quick for a single image, but would be cumbersome for analyzing an entire dataset with hundreds or thousands of images. The watershed methods did have quite high accuracy; the highest out of all of the methods tested. 

For machine learning techniques, I first had to create training and testing data. I did this by segmenting regions using the procedure I created in the classical image analysis, then built a feature matrix with 12 descriptors per particle such as equivalent diameter, edge ratio, and texture. I then used a Random Forest to choose the top 7 discriminative features, and labeled regions into "small" vs "large" particles based on whether their area was smaller or greater than the mean particle area. I then conducted training of the SVM and Random Forest models and analyzed their precision, recall, F1 scores, and confusion matrices. I finally performed unsupervised k-Means clustering, visualized the data using PCA, and calculated the silhouette scores for k values of 3, 5, and 7.

I found SVM and Random Forest to be efficient once created, with among the lowest runtimes (~Less than a second). However, since these are supervised techniques, I had to spend a significant amount of effort creating training and testing datasets. But, once they are created, these techniques can very efficiently extract features from large datasets. They also had very high accuracies, comparable to the classical watershed technique. The unsupervised technique, k-Means, had a slightly slower runtime (~7 seconds), but some time was saved by not having to create any training data. However, it's silhouette score was quite low, around 0.31, even for the best k value of k=7. I would still choose SVM and Random Forest over KMeans for accuracy and to analyze larger datasets more efficiently.

To create training datasets for Deep Learning, I augmented 20 images from DOPAD with rotations, flips, zoom, intensity shifts, and noise. I then built a CNN network, trained it on labeled data, and plotted the learning curves. I then made a U-Net segmentation model and trained it with binary cross-entropy loss and evaluated IoU and Dice coefficient. 

Deep Learning was definitely the most time-consuming method to create and train. The runtimes for both CNN and U-Net were much higher than other methods tested, and creating the training data was time consuming because of the augmentation requirements. The CNN had an F1 score of 0.75, while the U-Net had an IoU of 0.42 and Dice Coefficient of 0.59. Overall, the CNN was better, but still worse in my opinion than SVM and Random Forest whose accuracies were extremely high. However, if I did not have much data, I would use the CNN Deep Learning model with augmented images to classify data.